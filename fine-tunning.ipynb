{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Генерация текста с помощью модели GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель: sberbank-ai/rugpt3medium_based_on_gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, AdamW\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import random\n",
    "import textwrap\n",
    "\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import transformers\n",
    "\n",
    "if torch.cuda.is_available():    \n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('sberbank-ai/rugpt3medium_based_on_gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# путь к файлу с текстом\n",
    "PATH_TEXT = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PATH_TEXT, encoding='cp1251') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# text = re.sub('\\n{2,}', '\\n', text)\n",
    "# text = re.sub('[^A-Za-z0-9]+', '', text)\n",
    "# text = re.sub(\"\\d+\", \"\", text)\n",
    "# text = re.sub(r'[^\\w]', ' ', text)\n",
    "# text = text.replace('=','')\n",
    "# text = text.replace('[','')\n",
    "# text = text.replace(']','')\n",
    "# text = text.replace('\"','')\n",
    "# text = text.replace(\"'\",'')\n",
    "# text = text.replace('  ',' ')\n",
    "\n",
    "# print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.encode(text, add_special_tokens=True)\n",
    "tokens = np.array(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = len(tokens)//15\n",
    "train = []\n",
    "test = []\n",
    "for i in range(15):\n",
    "    if i%5 > 0:\n",
    "        train.extend(tokens[i*l: (i+1)*l])\n",
    "    else:\n",
    "        test.extend(tokens[i*l: (i+1)*l])\n",
    "train = np.array(train)\n",
    "test = np.array(test)\n",
    "\n",
    "print(len(tokens), len(train), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained(\n",
    "    'sberbank-ai/rugpt3medium_based_on_gpt2',\n",
    "    output_attentions = False,\n",
    "    output_hidden_states = False,\n",
    "    )\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "max_len = 256\n",
    "epochs = 7\n",
    "\n",
    "n_train = len(train)//(batch_size*max_len)\n",
    "n_test = len(test)//(batch_size*max_len)\n",
    "print(n_train, n_test)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr = 1e-5, eps = 1e-8)\n",
    "\n",
    "total_steps = n_train * epochs\n",
    "scheduler = transformers.get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)\n",
    "\n",
    "\n",
    "def accuracy(y_true, logits):\n",
    "    return torch.mean((y_true[1:] == torch.argmax(logits, dim=2)[:-1]).float()).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_tensors(x, i, batch_size=batch_size, max_len=max_len):\n",
    "    batch_ids = x[i*batch_size*max_len: (i+1)*batch_size*max_len]\n",
    "    batch_ids = batch_ids.reshape(batch_size, max_len)\n",
    "    batch_ids = torch.tensor(batch_ids).to(device)\n",
    "    return batch_ids\n",
    "\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    print(f'epoch {epoch}/{epochs} : training')\n",
    "\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    model.train()\n",
    "    pbar = tqdm(range(n_train))\n",
    "    for i in pbar:\n",
    "        batch_ids = prep_tensors(train, i)\n",
    "\n",
    "        model.zero_grad()\n",
    "        loss, logits, _ = model(batch_ids,\n",
    "                             token_type_ids=None, \n",
    "                             labels=batch_ids\n",
    "                             ).values()\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        train_loss.append(loss.item())\n",
    "        train_acc.append(accuracy(batch_ids, logits))\n",
    "        pbar.set_description(f'acc {np.mean(train_acc):.4f} loss {np.mean(train_loss):.4f}', refresh=True)\n",
    "\n",
    "    \n",
    "    print(f'epoch {epoch}/{epochs} : validation')\n",
    "    model.eval()\n",
    "    val_acc = []\n",
    "    val_loss = []\n",
    "    pbar = tqdm(range(n_test))\n",
    "    for i in pbar:\n",
    "        batch_ids = prep_tensors(test, i)\n",
    "        with torch.no_grad():        \n",
    "            loss, logits, _ = model(batch_ids, \n",
    "                                token_type_ids=None, \n",
    "                                labels=batch_ids\n",
    "                                 ).values()\n",
    "        \n",
    "        val_loss.append(loss.item())\n",
    "        val_acc.append(accuracy(batch_ids, logits))\n",
    "        pbar.set_description(f'acc {np.mean(val_acc):.4f} loss {np.mean(val_loss):.4f}', refresh=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt, len_gen=20, temperature=1):\n",
    "    generated = tokenizer.encode(prompt)\n",
    "    context = torch.tensor([generated]).to(device)\n",
    "    past = None\n",
    "\n",
    "    for i in tqdm(range(len_gen)):\n",
    "        output, past = model(context, past_key_values=past).values()\n",
    "        output = output / temperature\n",
    "        token = torch.distributions.Categorical(logits=output[..., -1, :]).sample()\n",
    "        \n",
    "        generated += token.tolist()\n",
    "        context = token.unsqueeze(0)\n",
    "\n",
    "    sequence = tokenizer.decode(generated)\n",
    "\n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gtp_space(text):\n",
    "  prompt = text\n",
    "  prompt = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "  out = model.generate(\n",
    "      input_ids=prompt,\n",
    "      max_length=200,\n",
    "      num_beams=6,\n",
    "      do_sample=True,\n",
    "      temperature=1.,\n",
    "      top_k=50,\n",
    "      top_p=0.7,\n",
    "      no_repeat_ngram_size=4,\n",
    "      num_return_sequences=1,\n",
    "      ).cpu().numpy()\n",
    "  for out_ in out:\n",
    "      text_gen = textwrap.fill(tokenizer.decode(out_), 120)\n",
    "  text_gen = text_gen.replace('\\xa0',' ')\n",
    "  text_gen = text_gen.replace('\\n',' ')\n",
    "  text_gen = text_gen[:text_gen.rfind('.')+1]\n",
    "  text_gen = re.sub(r\"(\\.\\s+|^)(\\w+)\",\n",
    "                  lambda m: m.group(1) + m.group(2).capitalize(),\n",
    "                  text_gen)\n",
    "  return text_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Снизошел'\n",
    "gtp_space(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'modelpop.pt'\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH='modelpop.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import textwrap\n",
    "import numpy as np\n",
    "import re\n",
    "import textwrap\n",
    "from transformers import GPT2LMHeadModel, AdamW\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# device = 'cpu'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\n",
    "    'sberbank-ai/rugpt3medium_based_on_gpt2',\n",
    "    output_attentions = False,\n",
    "    output_hidden_states = False,\n",
    "    state_dict=torch.load(PATH, map_location=torch.device(device))\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "#Load the model & tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('sberbank-ai/rugpt3medium_based_on_gpt2')\n",
    "\n",
    "def generate(prompt, len_gen=20, temperature=1):\n",
    "    generated = tokenizer.encode(prompt)\n",
    "    context = torch.tensor([generated]).to(device)\n",
    "    past = None\n",
    "\n",
    "    for i in range(len_gen):\n",
    "        output, past = model(context, past_key_values=past).values()\n",
    "        output = output / temperature\n",
    "        token = torch.distributions.Categorical(logits=output[..., -1, :]).sample()\n",
    "        generated += token.tolist()\n",
    "        context = token.unsqueeze(0)\n",
    "\n",
    "    sequence = tokenizer.decode(generated)\n",
    "    return sequence\n",
    "\n",
    "def gtp_space(text):\n",
    "    prompt = text\n",
    "    prompt = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    with torch.no_grad():\n",
    "      out = model.generate(input_ids=prompt,\n",
    "          max_length=200,\n",
    "          num_beams=6,\n",
    "          do_sample=True,\n",
    "          temperature=1.,\n",
    "          top_k=50,\n",
    "          top_p=0.7,\n",
    "          no_repeat_ngram_size=4,\n",
    "          num_return_sequences=1,\n",
    "          ).cpu().numpy()\n",
    "    for out_ in out:\n",
    "        text_gen = textwrap.fill(tokenizer.decode(out_), 120)\n",
    "        text_gen = text_gen.replace('\\xa0',' ')\n",
    "        text_gen = text_gen.replace('\\n',' ')\n",
    "        text_gen = text_gen[:text_gen.rfind('.')+1]\n",
    "        text_gen = re.sub(r\"(\\.\\s+|^)(\\w+)\", lambda m: m.group(1) + m.group(2).capitalize(), text_gen)\n",
    "    return text_gen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Послание'\n",
    "gtp_space(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вариант локального бота для телеги"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "from pathlib import Path\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from aiogram import Bot, types\n",
    "from aiogram.dispatcher import Dispatcher\n",
    "from aiogram.utils import executor\n",
    "from aiogram.types import ContentType, File, Message\n",
    "from bs4 import BeautifulSoup\n",
    "from subprocess import call\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN = \"\" # Токен для бота (получаем через @BotFather)\n",
    "\n",
    "os.getenv(\"TOKEN\")\n",
    "\n",
    "# Инициализация бота\n",
    "bot = Bot(token=TOKEN)\n",
    "dp = Dispatcher(bot)\n",
    "\n",
    "@dp.message_handler(commands=['start'])\n",
    "async def start(message: types.Message):\n",
    "    user_name = message.from_user.full_name\n",
    "    user_id = message.from_user.id\n",
    "    await message.reply(f'Hello {user_name}! Your user_id = {user_id}')\n",
    "\n",
    "@dp.message_handler()\n",
    "async def echo(message: types.Message):\n",
    "    query = message.text\n",
    "    text_gen = gtp_space(query)\n",
    "    await message.reply(text_gen)\n",
    "\n",
    "# Команда запуска бота\n",
    "if __name__ == '__main__':\n",
    "    executor.start_polling(dp)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
